<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Synopsis</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h2>Synopsis</h2>

<p>This paper will attempt to build a machine learning model using the &#39;Weight Lifting Dataset&#39; in order to predict the efficacy with which a subject performs a weight lifting exercise.</p>

<p>The interested reader is referred to <a href="http://groupware.les.inf.puc-rio.br/har">http://groupware.les.inf.puc-rio.br/har</a> for further information on this dataset and it&#39;s characteristics.</p>

<h2>Data Input</h2>

<p>We define here a generic function which will download a web based resource optionally unzipping if it required</p>

<pre><code class="r">dataLoader &lt;- function(vDataFile, vSourceURL, unzip = FALSE){    
    if(!file.exists(vDataFile))
        {
        vDestFile = basename(vSourceURL)
        message(&quot;Downloading data file - please wait ... &quot;, appendLF = FALSE)
        download.file(url = vSourceURL, destfile = vDestFile, method = &quot;curl&quot;, quiet = TRUE)
        message(&quot;Done&quot;, appendLF = TRUE)
        if(unzip == TRUE)
            {
            message(&quot;Unzipping data file - please wait ... &quot;, appendLF = FALSE)
            unzip(vDestFile, overwrite = TRUE)
            }
        message(&quot;Done&quot;, appendLF = TRUE)
        }
    }
</code></pre>

<p>The above defined function is used to download the activity.csv data from 
<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<pre><code class="r">dataLoader(&quot;pml-training.csv&quot;, &quot;https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv&quot;, unzip = FALSE)
df.rawData &lt;- read.csv(&quot;pml-training.csv&quot;, header = TRUE, stringsAsFactors = TRUE, na.strings = &quot;NA&quot;)

# Throw away the dataLoader function - it&#39;s not needed any longer
rm(&quot;dataLoader&quot;)
</code></pre>

<p>Load the libraries required for this anaysis</p>

<pre><code class="r">require(caret)
require(randomForest)
</code></pre>

<h2>Exploratory Analysis</h2>

<p>An inspection of the data frame dimensions indicates that the available data contains nearly 20,000 rows of 160 columns. This is a substantial number of predictors with which to build a model, so some reduction techniques will be applied to select a smaller set of predictors.</p>

<p>The various types and numbers of column classes are as follows.</p>

<pre><code class="r">with(df.rawData, {
    print(dim(df.rawData))
    for(v.type in unique(unname(sapply(df.rawData, class)))) {
        print(paste(v.type, sum(sum(unname(sapply(df.rawData, class)) %in% v.type))))
    }
})
</code></pre>

<pre><code>## [1] 19622   160
## [1] &quot;integer 35&quot;
## [1] &quot;factor 37&quot;
## [1] &quot;numeric 88&quot;
</code></pre>

<h2>Cross Validation Technique</h2>

<p>The cross validation technique selected is to split the available data into train and test sets. A 60/40 split is used. K-fold cross validation will be applied to the training dataset and the actual out-of-sample error will be estimated with the test set.</p>

<pre><code class="r">set.seed(1)
with(df.rawData, {
    v.trainingIndices &lt;- createDataPartition(y = df.rawData$classe, p = 0.6, list = FALSE)
    df.trainData &lt;&lt;- df.rawData[v.trainingIndices,]
    df.testData &lt;&lt;- df.rawData[-v.trainingIndices,]
})

# Throw away the original raw data frame - it&#39;s not needed any longer
rm(&quot;df.rawData&quot;)
</code></pre>

<h2>Covariate Selection</h2>

<p>A visual inspection of the <strong>train</strong> data set with head and str commands (outputs not shown for brevity) reveal some areas of concern in the data which will impair the effectiveness of any training algorithm.</p>

<ul>
<li>There are several columns where the datum type is clearly <em>not</em> useful for training purposes - e.g. &#39;user_name&#39; or &#39;raw_timestamp_part_n&#39;</li>
<li>There are several columns with a high number of NA values</li>
<li>There are columns with a high number of &#39;#DIV/0!&#39; or empty values</li>
</ul>

<p>Other areas of concern are</p>

<ul>
<li>There <em>may</em> be columns with low overall variance in the data - limiting their usefulness for training purposes</li>
<li>There <em>may</em> be columns that are highly correlated to each other -  rendering some columns redundant for training purposes</li>
</ul>

<p>The reader might question the wisdom of removing the raw_timestamp_part_n columns. The model is ultimately intended to evaluate the efficacy of a given action performed by a subject. The very fact that there are many time-adjacent data samples available each second, for each subject, would lend weight to the idea that analysing a time-sequenced group of samples could be of service in building an accurate and useful model. However the available data is classified on <strong>every available row</strong> - lending weight to the idea that single point-in-time samples are also of value when training.</p>

<p>Additionally, since the final test set contains <em>no</em> time adjacent samples and contains only single point-in-time measurements, the potential benefits of time sequence based analysis are rendered null and void. Am I cheating by having an insight into the final testing methodology and using this to guide my covariate selection process ? - I&#39;ll leave that up to you.</p>

<pre><code class="r"># Clean the column names to enforce tidy names under R&#39;s column naming conventions
names(df.trainData) &lt;- make.names(names(df.trainData), unique = TRUE)
names(df.testData) &lt;- make.names(names(df.testData), unique = TRUE)
</code></pre>

<pre><code class="r"># Drop this columns where &#39;common sense&#39; indicates that the datum type is simply not useful for training a model.
# Train and test sets receive the same treatment
with (df.trainData, {
    v.columnNamesToDrop &lt;- c(&quot;X&quot;, &quot;user_name&quot;, &quot;raw_timestamp_part_1&quot;, &quot;raw_timestamp_part_2&quot;, &quot;cvtd_timestamp&quot;, &quot;new_window&quot;, &quot;num_window&quot;)
    print(paste(&quot;The following training set column will be dropped : &quot;, v.columnNamesToDrop))
    df.trainData &lt;&lt;- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData &lt;&lt;- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
</code></pre>

<pre><code>## [1] &quot;The following training set column will be dropped :  X&quot;                   
## [2] &quot;The following training set column will be dropped :  user_name&quot;           
## [3] &quot;The following training set column will be dropped :  raw_timestamp_part_1&quot;
## [4] &quot;The following training set column will be dropped :  raw_timestamp_part_2&quot;
## [5] &quot;The following training set column will be dropped :  cvtd_timestamp&quot;      
## [6] &quot;The following training set column will be dropped :  new_window&quot;          
## [7] &quot;The following training set column will be dropped :  num_window&quot;
</code></pre>

<pre><code class="r"># Those columns from the *train* set where more than 50% of values are NA. The columns selected here are also removed from the test set
with (df.trainData, {
    v.naThreshold &lt;- dim(df.trainData)[1] * 0.5    
    v.naCounts &lt;- sapply(df.trainData, function(column) {
        sum(length(which(is.na(column))))
    })
    v.columnNamesToDrop &lt;- names(which(v.naCounts &gt; v.naThreshold))
    print(paste(&quot;The following training set column will be dropped : &quot;, v.columnNamesToDrop))
    df.trainData &lt;&lt;- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData &lt;&lt;- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
</code></pre>

<pre><code>##  [1] &quot;The following training set column will be dropped :  max_roll_belt&quot;           
##  [2] &quot;The following training set column will be dropped :  max_picth_belt&quot;          
##  [3] &quot;The following training set column will be dropped :  min_roll_belt&quot;           
##  [4] &quot;The following training set column will be dropped :  min_pitch_belt&quot;          
##  [5] &quot;The following training set column will be dropped :  amplitude_roll_belt&quot;     
##  [6] &quot;The following training set column will be dropped :  amplitude_pitch_belt&quot;    
##  [7] &quot;The following training set column will be dropped :  var_total_accel_belt&quot;    
##  [8] &quot;The following training set column will be dropped :  avg_roll_belt&quot;           
##  [9] &quot;The following training set column will be dropped :  stddev_roll_belt&quot;        
## [10] &quot;The following training set column will be dropped :  var_roll_belt&quot;           
## [11] &quot;The following training set column will be dropped :  avg_pitch_belt&quot;          
## [12] &quot;The following training set column will be dropped :  stddev_pitch_belt&quot;       
## [13] &quot;The following training set column will be dropped :  var_pitch_belt&quot;          
## [14] &quot;The following training set column will be dropped :  avg_yaw_belt&quot;            
## [15] &quot;The following training set column will be dropped :  stddev_yaw_belt&quot;         
## [16] &quot;The following training set column will be dropped :  var_yaw_belt&quot;            
## [17] &quot;The following training set column will be dropped :  var_accel_arm&quot;           
## [18] &quot;The following training set column will be dropped :  avg_roll_arm&quot;            
## [19] &quot;The following training set column will be dropped :  stddev_roll_arm&quot;         
## [20] &quot;The following training set column will be dropped :  var_roll_arm&quot;            
## [21] &quot;The following training set column will be dropped :  avg_pitch_arm&quot;           
## [22] &quot;The following training set column will be dropped :  stddev_pitch_arm&quot;        
## [23] &quot;The following training set column will be dropped :  var_pitch_arm&quot;           
## [24] &quot;The following training set column will be dropped :  avg_yaw_arm&quot;             
## [25] &quot;The following training set column will be dropped :  stddev_yaw_arm&quot;          
## [26] &quot;The following training set column will be dropped :  var_yaw_arm&quot;             
## [27] &quot;The following training set column will be dropped :  max_roll_arm&quot;            
## [28] &quot;The following training set column will be dropped :  max_picth_arm&quot;           
## [29] &quot;The following training set column will be dropped :  max_yaw_arm&quot;             
## [30] &quot;The following training set column will be dropped :  min_roll_arm&quot;            
## [31] &quot;The following training set column will be dropped :  min_pitch_arm&quot;           
## [32] &quot;The following training set column will be dropped :  min_yaw_arm&quot;             
## [33] &quot;The following training set column will be dropped :  amplitude_roll_arm&quot;      
## [34] &quot;The following training set column will be dropped :  amplitude_pitch_arm&quot;     
## [35] &quot;The following training set column will be dropped :  amplitude_yaw_arm&quot;       
## [36] &quot;The following training set column will be dropped :  max_roll_dumbbell&quot;       
## [37] &quot;The following training set column will be dropped :  max_picth_dumbbell&quot;      
## [38] &quot;The following training set column will be dropped :  min_roll_dumbbell&quot;       
## [39] &quot;The following training set column will be dropped :  min_pitch_dumbbell&quot;      
## [40] &quot;The following training set column will be dropped :  amplitude_roll_dumbbell&quot; 
## [41] &quot;The following training set column will be dropped :  amplitude_pitch_dumbbell&quot;
## [42] &quot;The following training set column will be dropped :  var_accel_dumbbell&quot;      
## [43] &quot;The following training set column will be dropped :  avg_roll_dumbbell&quot;       
## [44] &quot;The following training set column will be dropped :  stddev_roll_dumbbell&quot;    
## [45] &quot;The following training set column will be dropped :  var_roll_dumbbell&quot;       
## [46] &quot;The following training set column will be dropped :  avg_pitch_dumbbell&quot;      
## [47] &quot;The following training set column will be dropped :  stddev_pitch_dumbbell&quot;   
## [48] &quot;The following training set column will be dropped :  var_pitch_dumbbell&quot;      
## [49] &quot;The following training set column will be dropped :  avg_yaw_dumbbell&quot;        
## [50] &quot;The following training set column will be dropped :  stddev_yaw_dumbbell&quot;     
## [51] &quot;The following training set column will be dropped :  var_yaw_dumbbell&quot;        
## [52] &quot;The following training set column will be dropped :  max_roll_forearm&quot;        
## [53] &quot;The following training set column will be dropped :  max_picth_forearm&quot;       
## [54] &quot;The following training set column will be dropped :  min_roll_forearm&quot;        
## [55] &quot;The following training set column will be dropped :  min_pitch_forearm&quot;       
## [56] &quot;The following training set column will be dropped :  amplitude_roll_forearm&quot;  
## [57] &quot;The following training set column will be dropped :  amplitude_pitch_forearm&quot; 
## [58] &quot;The following training set column will be dropped :  var_accel_forearm&quot;       
## [59] &quot;The following training set column will be dropped :  avg_roll_forearm&quot;        
## [60] &quot;The following training set column will be dropped :  stddev_roll_forearm&quot;     
## [61] &quot;The following training set column will be dropped :  var_roll_forearm&quot;        
## [62] &quot;The following training set column will be dropped :  avg_pitch_forearm&quot;       
## [63] &quot;The following training set column will be dropped :  stddev_pitch_forearm&quot;    
## [64] &quot;The following training set column will be dropped :  var_pitch_forearm&quot;       
## [65] &quot;The following training set column will be dropped :  avg_yaw_forearm&quot;         
## [66] &quot;The following training set column will be dropped :  stddev_yaw_forearm&quot;      
## [67] &quot;The following training set column will be dropped :  var_yaw_forearm&quot;
</code></pre>

<pre><code class="r"># Those columns from the *train*  set where more than 50% of values are empty or contain &#39;#DIV/0!&#39;
with (df.trainData, {
    v.naThreshold &lt;- dim(df.trainData[1]) * 0.5
    v.naCounts &lt;- sapply(df.trainData, function(column) {
        length(grep(&quot;^$|#DIV/0!&quot;, column))
    })
    v.columnNamesToDrop &lt;- names(which(v.naCounts &gt; v.naThreshold))
    print(paste(&quot;The following training set column will be dropped : &quot;, v.columnNamesToDrop))
    df.trainData &lt;&lt;- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData &lt;&lt;- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
</code></pre>

<pre><code>##  [1] &quot;The following training set column will be dropped :  kurtosis_roll_belt&quot;     
##  [2] &quot;The following training set column will be dropped :  kurtosis_picth_belt&quot;    
##  [3] &quot;The following training set column will be dropped :  kurtosis_yaw_belt&quot;      
##  [4] &quot;The following training set column will be dropped :  skewness_roll_belt&quot;     
##  [5] &quot;The following training set column will be dropped :  skewness_roll_belt.1&quot;   
##  [6] &quot;The following training set column will be dropped :  skewness_yaw_belt&quot;      
##  [7] &quot;The following training set column will be dropped :  max_yaw_belt&quot;           
##  [8] &quot;The following training set column will be dropped :  min_yaw_belt&quot;           
##  [9] &quot;The following training set column will be dropped :  amplitude_yaw_belt&quot;     
## [10] &quot;The following training set column will be dropped :  kurtosis_roll_arm&quot;      
## [11] &quot;The following training set column will be dropped :  kurtosis_picth_arm&quot;     
## [12] &quot;The following training set column will be dropped :  kurtosis_yaw_arm&quot;       
## [13] &quot;The following training set column will be dropped :  skewness_roll_arm&quot;      
## [14] &quot;The following training set column will be dropped :  skewness_pitch_arm&quot;     
## [15] &quot;The following training set column will be dropped :  skewness_yaw_arm&quot;       
## [16] &quot;The following training set column will be dropped :  kurtosis_roll_dumbbell&quot; 
## [17] &quot;The following training set column will be dropped :  kurtosis_picth_dumbbell&quot;
## [18] &quot;The following training set column will be dropped :  kurtosis_yaw_dumbbell&quot;  
## [19] &quot;The following training set column will be dropped :  skewness_roll_dumbbell&quot; 
## [20] &quot;The following training set column will be dropped :  skewness_pitch_dumbbell&quot;
## [21] &quot;The following training set column will be dropped :  skewness_yaw_dumbbell&quot;  
## [22] &quot;The following training set column will be dropped :  max_yaw_dumbbell&quot;       
## [23] &quot;The following training set column will be dropped :  min_yaw_dumbbell&quot;       
## [24] &quot;The following training set column will be dropped :  amplitude_yaw_dumbbell&quot; 
## [25] &quot;The following training set column will be dropped :  kurtosis_roll_forearm&quot;  
## [26] &quot;The following training set column will be dropped :  kurtosis_picth_forearm&quot; 
## [27] &quot;The following training set column will be dropped :  kurtosis_yaw_forearm&quot;   
## [28] &quot;The following training set column will be dropped :  skewness_roll_forearm&quot;  
## [29] &quot;The following training set column will be dropped :  skewness_pitch_forearm&quot; 
## [30] &quot;The following training set column will be dropped :  skewness_yaw_forearm&quot;   
## [31] &quot;The following training set column will be dropped :  max_yaw_forearm&quot;        
## [32] &quot;The following training set column will be dropped :  min_yaw_forearm&quot;        
## [33] &quot;The following training set column will be dropped :  amplitude_yaw_forearm&quot;
</code></pre>

<p>Those columns in the <strong>train</strong> data set that exhibit a high correlation with each other indicate that some columns may be removed without compromising the usefulness of the training data for modelling purposes. The goal is to capture as much information as possible in as few columns as possible. The code below will determine the correlation between all columns of the training data and save this in a matrix. Printing those column / row indices of the matrix entries of interest will yield a set of row indices from the training data set which may be removed. An (arbitrary) threshold of 0.8 has been chosen to indicate &#39;high&#39; correlation. </p>

<pre><code class="r"># Those columns from the training data set which correlation is high (&gt; 0.8)
with(df.trainData, {
    # Build the matrix of relative correlations between all columns (except the &#39;classe&#39; column)
    correlationMatrix &lt;- abs(cor(df.trainData[, -dim(df.trainData)[2]]))
    # Zero the diagonal &#39;self correlation&#39; entries (always 1)
    diag(correlationMatrix) &lt;- 0
    # Capture the matrix indices in a vector and split into colum and row sets
    # We only need remove *one* of these sets, since the column / row values are transposed for the other sets of co-ordinates
    v.indices &lt;- as.numeric(unlist(strsplit(as.character(which(correlationMatrix &gt; 0.8, arr.ind = T)), &#39;\\s+&#39;)))
    v.columnIndicesToDrop &lt;- sort(unique(v.indices[1:(length(v.indices) / 2)]), decreasing = FALSE)
    print(paste(&quot;The following training set column will be dropped : &quot;, names(df.trainData)[v.columnIndicesToDrop]))
    # Get the names of the train (and test) columns to drop
    v.columnNamesToDrop &lt;- names(df.trainData)[v.columnIndicesToDrop]
    df.trainData &lt;&lt;- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData &lt;&lt;- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
</code></pre>

<pre><code>##  [1] &quot;The following training set column will be dropped :  roll_belt&quot;       
##  [2] &quot;The following training set column will be dropped :  pitch_belt&quot;      
##  [3] &quot;The following training set column will be dropped :  yaw_belt&quot;        
##  [4] &quot;The following training set column will be dropped :  total_accel_belt&quot;
##  [5] &quot;The following training set column will be dropped :  accel_belt_x&quot;    
##  [6] &quot;The following training set column will be dropped :  accel_belt_y&quot;    
##  [7] &quot;The following training set column will be dropped :  accel_belt_z&quot;    
##  [8] &quot;The following training set column will be dropped :  magnet_belt_x&quot;   
##  [9] &quot;The following training set column will be dropped :  gyros_arm_x&quot;     
## [10] &quot;The following training set column will be dropped :  gyros_arm_y&quot;     
## [11] &quot;The following training set column will be dropped :  accel_arm_x&quot;     
## [12] &quot;The following training set column will be dropped :  magnet_arm_x&quot;    
## [13] &quot;The following training set column will be dropped :  magnet_arm_y&quot;    
## [14] &quot;The following training set column will be dropped :  magnet_arm_z&quot;    
## [15] &quot;The following training set column will be dropped :  pitch_dumbbell&quot;  
## [16] &quot;The following training set column will be dropped :  yaw_dumbbell&quot;    
## [17] &quot;The following training set column will be dropped :  gyros_dumbbell_x&quot;
## [18] &quot;The following training set column will be dropped :  gyros_dumbbell_z&quot;
## [19] &quot;The following training set column will be dropped :  accel_dumbbell_x&quot;
## [20] &quot;The following training set column will be dropped :  accel_dumbbell_z&quot;
## [21] &quot;The following training set column will be dropped :  gyros_forearm_y&quot; 
## [22] &quot;The following training set column will be dropped :  gyros_forearm_z&quot;
</code></pre>

<p>Finally those columns of the <strong>train</strong> data sets which exhibit minimal variance many be excluded - since these contain less useful information for model building than a high variance column. The variance of each of the training set columns is computed and the 20th percentile is calculated.  Columns whose total variance is less than the 20th percentile are deemed not variable enough and are removed from the training and test sets.</p>

<pre><code class="r"># Those columns from the *train* data set where there is minimal variance in the data
with(df.trainData, {
    v.variance &lt;- vector(mode = &quot;numeric&quot;, length = 0)
    # v.threshold &lt;- vector(mode = &quot;numeric&quot;, length = 0)
    for(index in 1:dim(df.trainData)[2] - 1) {
      v.variance &lt;- c(v.variance, var(df.trainData[,index]))          
  }
  v.threshold &lt;- (quantile(v.variance, 0.2))
  v.columnNamesToDrop &lt;- names(df.trainData[which(v.variance &lt; v.threshold)])
  print(paste(&quot;The following training set column will be dropped : &quot;, v.columnNamesToDrop))
  df.trainData &lt;&lt;- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
  df.testData &lt;&lt;- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
</code></pre>

<pre><code>## [1] &quot;The following training set column will be dropped :  gyros_belt_x&quot;    
## [2] &quot;The following training set column will be dropped :  gyros_belt_y&quot;    
## [3] &quot;The following training set column will be dropped :  gyros_belt_z&quot;    
## [4] &quot;The following training set column will be dropped :  gyros_arm_z&quot;     
## [5] &quot;The following training set column will be dropped :  gyros_dumbbell_y&quot;
## [6] &quot;The following training set column will be dropped :  gyros_forearm_x&quot;
</code></pre>

<p>Now the final set of columns which are selected as the basis for building the model are available - these are :</p>

<pre><code class="r">print(paste(&quot;The following training set column will be included in the model : &quot;, names(df.trainData)))
</code></pre>

<pre><code>##  [1] &quot;The following training set column will be included in the model :  magnet_belt_y&quot;       
##  [2] &quot;The following training set column will be included in the model :  magnet_belt_z&quot;       
##  [3] &quot;The following training set column will be included in the model :  roll_arm&quot;            
##  [4] &quot;The following training set column will be included in the model :  pitch_arm&quot;           
##  [5] &quot;The following training set column will be included in the model :  yaw_arm&quot;             
##  [6] &quot;The following training set column will be included in the model :  total_accel_arm&quot;     
##  [7] &quot;The following training set column will be included in the model :  accel_arm_y&quot;         
##  [8] &quot;The following training set column will be included in the model :  accel_arm_z&quot;         
##  [9] &quot;The following training set column will be included in the model :  roll_dumbbell&quot;       
## [10] &quot;The following training set column will be included in the model :  total_accel_dumbbell&quot;
## [11] &quot;The following training set column will be included in the model :  accel_dumbbell_y&quot;    
## [12] &quot;The following training set column will be included in the model :  magnet_dumbbell_x&quot;   
## [13] &quot;The following training set column will be included in the model :  magnet_dumbbell_y&quot;   
## [14] &quot;The following training set column will be included in the model :  magnet_dumbbell_z&quot;   
## [15] &quot;The following training set column will be included in the model :  roll_forearm&quot;        
## [16] &quot;The following training set column will be included in the model :  pitch_forearm&quot;       
## [17] &quot;The following training set column will be included in the model :  yaw_forearm&quot;         
## [18] &quot;The following training set column will be included in the model :  total_accel_forearm&quot; 
## [19] &quot;The following training set column will be included in the model :  accel_forearm_x&quot;     
## [20] &quot;The following training set column will be included in the model :  accel_forearm_y&quot;     
## [21] &quot;The following training set column will be included in the model :  accel_forearm_z&quot;     
## [22] &quot;The following training set column will be included in the model :  magnet_forearm_x&quot;    
## [23] &quot;The following training set column will be included in the model :  magnet_forearm_y&quot;    
## [24] &quot;The following training set column will be included in the model :  magnet_forearm_z&quot;    
## [25] &quot;The following training set column will be included in the model :  classe&quot;
</code></pre>

<h2>Missing Values Imputation</h2>

<p>The data set which will be used to generate the model is sufficiently clean that <strong>no</strong> missing values need be imputed.
A check of the total number of NA values in the training set confirms this.</p>

<pre><code class="r">sum(is.na(df.trainData))
</code></pre>

<pre><code>## [1] 0
</code></pre>

<h2>Model Training</h2>

<p>The technique of repeated K-fold cross validation has been selected for this exercise.  A value of 3 has been arbitrarily chosen for K. For the actual model building a random forest technique has been chosen.</p>

<pre><code class="r">trainOptions &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 3, repeats = 3, verboseIter = TRUE)
</code></pre>

<pre><code class="r">set.seed(1)
# model &lt;- train(classe ~ ., data = df.trainData, method = &quot;rf&quot;, trControl = trainOptions)
</code></pre>

<h2>Results</h2>

<p>A confusion matrix of the model applied to the test data gives an estimate of 97.8%  for the out of sample accuracy of the final model</p>

<pre><code class="r">confusionMatrix(df.testData$classe, predict(model, df.testData))
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 2226    3    2    1    0
##          B   32 1464   22    0    0
##          C    0   16 1350    2    0
##          D    0    1   51 1230    4
##          E    0    8    8   19 1407
## 
## Overall Statistics
##                                          
##                Accuracy : 0.9785         
##                  95% CI : (0.975, 0.9816)
##     No Information Rate : 0.2878         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.9727         
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9858   0.9812   0.9421   0.9824   0.9972
## Specificity            0.9989   0.9915   0.9972   0.9915   0.9946
## Pos Pred Value         0.9973   0.9644   0.9868   0.9565   0.9757
## Neg Pred Value         0.9943   0.9956   0.9872   0.9966   0.9994
## Prevalence             0.2878   0.1902   0.1826   0.1596   0.1798
## Detection Rate         0.2837   0.1866   0.1721   0.1568   0.1793
## Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
## Balanced Accuracy      0.9924   0.9864   0.9696   0.9870   0.9959
</code></pre>

<p>The output of &#39;varImp&#39; shows the relative importance of the variables used in the model</p>

<pre><code class="r">varImp(model)
</code></pre>

<pre><code>## rf variable importance
## 
##   only 20 most important variables shown (out of 24)
## 
##                      Overall
## magnet_dumbbell_z     100.00
## magnet_dumbbell_y      89.49
## pitch_forearm          81.55
## magnet_belt_y          79.31
## magnet_belt_z          79.25
## magnet_dumbbell_x      73.61
## roll_forearm           68.36
## roll_dumbbell          56.43
## accel_dumbbell_y       54.79
## roll_arm               44.33
## yaw_arm                35.62
## total_accel_dumbbell   35.28
## accel_forearm_x        32.98
## magnet_forearm_z       29.79
## accel_forearm_z        27.84
## magnet_forearm_y       22.02
## magnet_forearm_x       21.93
## accel_arm_y            20.30
## pitch_arm              19.92
## yaw_forearm            13.00
</code></pre>

<p>The finalModel is shown for reference purposes</p>

<pre><code class="r">model$finalModel
</code></pre>

<pre><code>## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 2.01%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3339    7    1    0    1 0.002688172
## B   51 2198   20    4    6 0.035541904
## C    0   31 2016    5    2 0.018500487
## D    0    0   72 1856    2 0.038341969
## E    2    7    2   24 2130 0.016166282
</code></pre>

<h2>Acknowledgements</h2>

<p>The author gratefully acknowlegdes the use of the &#39;Weight Lifting Exercise&#39; dataset
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human &#39;13) . Stuttgart, Germany: ACM SIGCHI, 2013.</p>

<p>Read more: <a href="http://groupware.les.inf.puc-rio.br/har#ixzz3gz0Re6YS">http://groupware.les.inf.puc-rio.br/har#ixzz3gz0Re6YS</a></p>

</body>

</html>
