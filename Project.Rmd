---
title: "Machine Learning Project"
author: "Sean Fitzpatrick"
date: "21-7-2015"
output: html_document
---

```{r echo=FALSE}
opts_chunk$set(echo = TRUE)
```

## Synopsis
This paper will attempt to build a machine learning model using the 'Weight Lifting Dataset' in order to predict the efficacy with which a subject performs a weight lifting exercise.

The interested reader is referred to http://groupware.les.inf.puc-rio.br/har for further information on this dataset and it's characteristics.

## Data Input
We define here a generic function which will download a web based resource optionally unzipping if it required

```{r define_dataloader_function}
dataLoader <- function(vDataFile, vSourceURL, unzip = FALSE){    
    if(!file.exists(vDataFile))
        {
        vDestFile = basename(vSourceURL)
        message("Downloading data file - please wait ... ", appendLF = FALSE)
        download.file(url = vSourceURL, destfile = vDestFile, method = "curl", quiet = TRUE)
        message("Done", appendLF = TRUE)
        if(unzip == TRUE)
            {
            message("Unzipping data file - please wait ... ", appendLF = FALSE)
            unzip(vDestFile, overwrite = TRUE)
            }
        message("Done", appendLF = TRUE)
        }
    }
```

The above defined function is used to download the activity.csv data from 
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

```{r message=FALSE}
dataLoader("pml-training.csv", "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", unzip = FALSE)
df.rawData <- read.csv("pml-training.csv", header = TRUE, stringsAsFactors = TRUE, na.strings = "NA")

# Throw away the dataLoader function - it's not needed any longer
rm("dataLoader")
```

Load the libraries required for this anaysis

```{r load_modules, message = FALSE}
require(caret)
require(randomForest)
```

## Exploratory Analysis
An inspection of the data frame dimensions indicates that the available data contains nearly 20,000 rows of 160 columns. This is a substantial number of predictors with which to build a model, so some reduction techniques will be applied to select a smaller set of predictors.

The various types and numbers of column classes are as follows.

```{r display_dataframe_characteristics}
with(df.rawData, {
    print(dim(df.rawData))
    for(v.type in unique(unname(sapply(df.rawData, class)))) {
        print(paste(v.type, sum(sum(unname(sapply(df.rawData, class)) %in% v.type))))
    }
})
```

## Cross Validation Technique
The cross validation technique selected is to split the available data into train and test sets. A 60/40 split is used. K-fold cross validation will be applied to the training dataset and the actual out-of-sample error will be estimated with the test set.

```{r partition_train_test_sets}
set.seed(1)
with(df.rawData, {
    v.trainingIndices <- createDataPartition(y = df.rawData$classe, p = 0.6, list = FALSE)
    df.trainData <<- df.rawData[v.trainingIndices,]
    df.testData <<- df.rawData[-v.trainingIndices,]
})

# Throw away the original raw data frame - it's not needed any longer
rm("df.rawData")
```

## Covariate Selection
A visual inspection of the **train** data set with head and str commands (outputs not shown for brevity) reveal some areas of concern in the data which will impair the effectiveness of any training algorithm.

- There are several columns where the datum type is clearly *not* useful for training purposes - e.g. 'user_name' or 'raw_timestamp_part_n'
- There are several columns with a high number of NA values
- There are columns with a high number of '#DIV/0!' or empty values

Other areas of concern are

- There *may* be columns with low overall variance in the data - limiting their usefulness for training purposes
- There *may* be columns that are highly correlated to each other -  rendering some columns redundant for training purposes

The reader might question the wisdom of removing the raw_timestamp_part_n columns. The model is ultimately intended to evaluate the efficacy of a given action performed by a subject. The very fact that there are many time-adjacent data samples available each second, for each subject, would lend weight to the idea that analysing a time-sequenced group of samples could be of service in building an accurate and useful model. However the available data is classified on **every available row** - lending weight to the idea that single point-in-time samples are also of value when training.

Additionally, since the final test set contains *no* time adjacent samples and contains only single point-in-time measurements, the potential benefits of time sequence based analysis are rendered null and void. Am I cheating by having an insight into the final testing methodology and using this to guide my covariate selection process ? - I'll leave that up to you.

```{r tidy_column_names}
# Clean the column names to enforce tidy names under R's column naming conventions
names(df.trainData) <- make.names(names(df.trainData), unique = TRUE)
names(df.testData) <- make.names(names(df.testData), unique = TRUE)
```


```{r remove_defunct_dataframe_columns_1}
# Drop this columns where 'common sense' indicates that the datum type is simply not useful for training a model.
# Train and test sets receive the same treatment
with (df.trainData, {
    v.columnNamesToDrop <- c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window")
    print(paste("The following training set column will be dropped : ", v.columnNamesToDrop))
    df.trainData <<- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData <<- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})

# Those columns from the *train* set where more than 50% of values are NA. The columns selected here are also removed from the test set
with (df.trainData, {
    v.naThreshold <- dim(df.trainData)[1] * 0.5    
    v.naCounts <- sapply(df.trainData, function(column) {
        sum(length(which(is.na(column))))
    })
    v.columnNamesToDrop <- names(which(v.naCounts > v.naThreshold))
    print(paste("The following training set column will be dropped : ", v.columnNamesToDrop))
    df.trainData <<- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData <<- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})

# Those columns from the *train*  set where more than 50% of values are empty or contain '#DIV/0!'
with (df.trainData, {
    v.naThreshold <- dim(df.trainData[1]) * 0.5
    v.naCounts <- sapply(df.trainData, function(column) {
        length(grep("^$|#DIV/0!", column))
    })
    v.columnNamesToDrop <- names(which(v.naCounts > v.naThreshold))
    print(paste("The following training set column will be dropped : ", v.columnNamesToDrop))
    df.trainData <<- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData <<- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
```

Those columns in the **train** data set that exhibit a high correlation with each other indicate that some columns may be removed without compromising the usefulness of the training data for modelling purposes. The goal is to capture as much information as possible in as few columns as possible. The code below will determine the correlation between all columns of the training data and save this in a matrix. Printing those column / row indices of the matrix entries of interest will yield a set of row indices from the training data set which may be removed. An (arbitrary) threshold of 0.8 has been chosen to indicate 'high' correlation. 

```{r remove_defunct_dataframe_columns_2}
# Those columns from the training data set which correlation is high (> 0.8)
with(df.trainData, {
    # Build the matrix of relative correlations between all columns (except the 'classe' column)
    correlationMatrix <- abs(cor(df.trainData[, -dim(df.trainData)[2]]))
    # Zero the diagonal 'self correlation' entries (always 1)
    diag(correlationMatrix) <- 0
    # Capture the matrix indices in a vector and split into colum and row sets
    # We only need remove *one* of these sets, since the column / row values are transposed for the other sets of co-ordinates
    v.indices <- as.numeric(unlist(strsplit(as.character(which(correlationMatrix > 0.8, arr.ind = T)), '\\s+')))
    v.columnIndicesToDrop <- sort(unique(v.indices[1:(length(v.indices) / 2)]), decreasing = FALSE)
    print(paste("The following training set column will be dropped : ", names(df.trainData)[v.columnIndicesToDrop]))
    # Get the names of the train (and test) columns to drop
    v.columnNamesToDrop <- names(df.trainData)[v.columnIndicesToDrop]
    df.trainData <<- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
    df.testData <<- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
```

Finally those columns of the **train** data sets which exhibit minimal variance many be excluded - since these contain less useful information for model building than a high variance column. The variance of each of the training set columns is computed and the 20th percentile is calculated.  Columns whose total variance is less than the 20th percentile are deemed not variable enough and are removed from the training and test sets.

```{r}
# Those columns from the *train* data set where there is minimal variance in the data
with(df.trainData, {
    v.variance <- vector(mode = "numeric", length = 0)
    # v.threshold <- vector(mode = "numeric", length = 0)
    for(index in 1:dim(df.trainData)[2] - 1) {
      v.variance <- c(v.variance, var(df.trainData[,index]))          
  }
  v.threshold <- (quantile(v.variance, 0.2))
  v.columnNamesToDrop <- names(df.trainData[which(v.variance < v.threshold)])
  print(paste("The following training set column will be dropped : ", v.columnNamesToDrop))
  df.trainData <<- df.trainData[,!(names(df.trainData) %in% v.columnNamesToDrop)]
  df.testData <<- df.testData[,!(names(df.testData) %in% v.columnNamesToDrop)]
})
```

Now the final set of columns which are selected as the basis for building the model are available - these are :

```{r print_model_covariates}
print(paste("The following training set column will be included in the model : ", names(df.trainData)))
```

## Missing Values Imputation
The data set which will be used to generate the model is sufficiently clean that **no** missing values need be imputed.
A check of the total number of NA values in the training set confirms this.

```{r check_final_na_count}
sum(is.na(df.trainData))
```

## Model Training
The technique of repeated K-fold cross validation has been selected for this exercise.  A value of 3 has been arbitrarily chosen for K. For the actual model building a random forest technique has been chosen.

```{r set_training_options}
trainOptions <- trainControl(method = "repeatedcv", number = 3, repeats = 3, verboseIter = TRUE)
```

```{r perform_training}
set.seed(1)
# model <- train(classe ~ ., data = df.trainData, method = "rf", trControl = trainOptions)
```

## Results

A confusion matrix of the model applied to the test data gives an estimate of 97.8%  for the out of sample accuracy of the final model

```{r print_confusion_matrix}
confusionMatrix(df.testData$classe, predict(model, df.testData))
```

The output of 'varImp' shows the relative importance of the variables used in the model

```{r show_varimp_output}
varImp(model)
```

The finalModel is shown for reference purposes

```{r show_final_model}
model$finalModel
```


## Acknowledgements

The author gratefully acknowlegdes the use of the 'Weight Lifting Exercise' dataset
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3gz0Re6YS
